[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "bwrob blog",
    "section": "",
    "text": "This blog is a battleground of sorts, but instead of swords and shields, we wield the weapons of Python and C++. I’m a mathematician turned quantitative analyst turned software engineer. You can expect high standard deviation of topics here.\nHere, I’ll document my coding conquests, from building practical and impractical tools, exploring financial concepts, to playing around with physics simulations.\nExpect a healthy dose of humor alongside the technical discussions. Let’s be honest, even the most complex problems are more enjoyable with a sprinkle of laughter. So, grab a cup of coffee and join me on this exploration – even if it’s just for one interested reader!\n\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nLinux in Windows via WSL\n\n\n\nDev Setup\n\n\n\nHow to have a great dev setup and still play CS over a coffee break.\n\n\n\nbwrob\n\n\nAug 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython Startup Script\n\n\n\nPythonic Distractions\n\n\n\nHow to start your day with a cup of coffee, not opening the same apps all over again.\n\n\n\nbwrob\n\n\nMay 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExit stack to the rescue\n\n\n\nPythonic Distractions\n\n\nContext Managers\n\n\n\nHow to chain resource managers in an elegant way.\n\n\n\nbwrob\n\n\nMay 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nStrategies for Numerical Integration\n\n\n\nPythonic Distractions\n\n\nDesign Patterns\n\n\n\nHow to apply strategy design pattern in Python.\n\n\n\nbwrob\n\n\nApr 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nNews\n\n\n\nHow to enjoy the ride with me.\n\n\n\nbwrob\n\n\nApr 23, 2024\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "posts/strategy-pattern-integration/index.html",
    "href": "posts/strategy-pattern-integration/index.html",
    "title": "Strategies for Numerical Integration",
    "section": "",
    "text": "Calculation of many financial methods or metrics relies on a mathematical tool called numerical integration. In simple terms, numerical integration takes a function that represents a continuous process (like the changing value of an investment over time) and approximates the area under its curve. This area can then be used to calculate important quantities, like the total return of the investment or price of a derivative instrument.\nSo we are tasked with a problem, and one that has many different ways of solving, or rather approximating the solution. You most likely (taking into account you’re still reading this) encountered rectangle rule, or Riemann summation in your Calculus 101 course/self-learning. But there are many other techniques, which we call schemes.\nFor quantitative analysts, the choice of the integration method matters. Different integration schemes offer varying levels of accuracy and efficiency. Unfortunately there’s no best technique. Same algorithm can be a perfect fit for problems with certain characteristics, but unusable for others. And then there is the old as time performance vs. accuracy trade-off.\nAs programmers working in finance, we need to be adaptable and leverage solutions that allow us to switch between these methods seamlessly."
  },
  {
    "objectID": "posts/strategy-pattern-integration/index.html#numerical-integration",
    "href": "posts/strategy-pattern-integration/index.html#numerical-integration",
    "title": "Strategies for Numerical Integration",
    "section": "",
    "text": "Calculation of many financial methods or metrics relies on a mathematical tool called numerical integration. In simple terms, numerical integration takes a function that represents a continuous process (like the changing value of an investment over time) and approximates the area under its curve. This area can then be used to calculate important quantities, like the total return of the investment or price of a derivative instrument.\nSo we are tasked with a problem, and one that has many different ways of solving, or rather approximating the solution. You most likely (taking into account you’re still reading this) encountered rectangle rule, or Riemann summation in your Calculus 101 course/self-learning. But there are many other techniques, which we call schemes.\nFor quantitative analysts, the choice of the integration method matters. Different integration schemes offer varying levels of accuracy and efficiency. Unfortunately there’s no best technique. Same algorithm can be a perfect fit for problems with certain characteristics, but unusable for others. And then there is the old as time performance vs. accuracy trade-off.\nAs programmers working in finance, we need to be adaptable and leverage solutions that allow us to switch between these methods seamlessly."
  },
  {
    "objectID": "posts/strategy-pattern-integration/index.html#design-patterns",
    "href": "posts/strategy-pattern-integration/index.html#design-patterns",
    "title": "Strategies for Numerical Integration",
    "section": "Design patterns",
    "text": "Design patterns\nThis is where design patterns come in. Design patterns are reusable solutions to common programming problems. Their widespread adoption in software development is largely attributed to the publication of Design Patterns: Elements of Reusable Object-Oriented Software in 1994. Authored by E. Gamma, R. Helm, R. Johnson, and J. Vlissides (often referred to as the “Gang of Four” or GoF), this book cataloged 23 essential software design patterns. These patterns provided solutions to common design problems in object-oriented programming, promoting code reusability, maintainability, and flexibility.\nSome design patterns can feel clunky or inelegant when implemented in Python. The language itself often has built-in features or idioms that achieve the same result in a more Pythonic way (meaning it follows Python’s style and conventions). Sometimes, design patterns can be seen as overcomplicating simple problems. On the other hand, usage of well-known and understood patterns may enhance your engineering skills and improve code readability.\nUltimately, the decision of whether or not to use design patterns in Python depends on the specific context of your project and your coding style. There’s no right or wrong answer. But first, you need to know the classics to diss the classics. We’ll hold on with the dissing for now, cause in the example below chosen design pattern makes for a very clean implementation. You’ll see for yourself."
  },
  {
    "objectID": "posts/strategy-pattern-integration/index.html#strategy-pattern",
    "href": "posts/strategy-pattern-integration/index.html#strategy-pattern",
    "title": "Strategies for Numerical Integration",
    "section": "Strategy pattern",
    "text": "Strategy pattern\nWe know the stage now — one problem statement, multiple strategies to tackle. Important observation here is that we don’t actually care which one is used. When you substitute the integral value to client code — a formula or further algorithm — it’s irrelevant how it was computed, as long its correct to required level of accuracy. This means that the problem should be decoupled from algorithms to solve it. We should target a implementation where you can state a problem Calculate the integral of \\(\\sin(x)\\) from \\(0\\) to \\(\\pi\\) and then just throw different algorithms at it to obtain a solution. So let’s get coding!\n\n\n\n\n\n\nNote\n\n\n\nI will show you this pattern through an example. If you prefer more generic setup see Refactoring Guru’s implementation. The customary ‘software engineering’ example used to present the SDP is sorting a list of integers using different sorting algorithms."
  },
  {
    "objectID": "posts/strategy-pattern-integration/index.html#abstract-schema",
    "href": "posts/strategy-pattern-integration/index.html#abstract-schema",
    "title": "Strategies for Numerical Integration",
    "section": "Abstract schema",
    "text": "Abstract schema\nEach scheme that we’d come up with, even the most complex ones, would have the same main purpose — ‘integrate’. To make the implementation for it, we create a template class that all concrete schemes will inherit from.\n\nfrom abc import ABC, abstractmethod\nfrom typing import Callable\n\nclass IntegrationScheme(ABC):\n    \"\"\"Abstract base class for integration schemas.\"\"\"\n\n    @abstractmethod\n    def integrate(\n        self,\n        integrand: Callable[[float], float],\n        *,\n        start: float,\n        end: float,\n    ) -&gt; float:\n        \"\"\"Abstract method for integrating a function.\"\"\"\n\nUnpacking this, we already used some nifty Pythonic tricks in those few lines:\n\nABC is a way of defining abstract classes. If you try to create an object of a class inheriting from ABC you’d get an error. It is used as a base class for concrete subclasses and serves as a template. Think of an example of animal and cat from the real world. You’ve never seen an abstract animal being in your life (that would be a truly transcendental experience). But you’ve hopefully seen many cats.\nDecorator @abstractmethod signifies that the method is just a mock-up. It needs to be present and overridden in all concrete classes that inherit from IntegrationScheme\nType annotations like start: float don’t affect the script behavior in any way. Those are only for us to not get lost in Python’s dynamic typing magic. They can also be leveraged by static type checkers like mypy to flag problems with your code before you run it — just like in compiled languages.\nCallable annotation signifies a function-like object something you can call through (), like some_func(one, second=two)’ — here some_func is a callable. Calls to an object can be implemented by writing the __call__ method for the class."
  },
  {
    "objectID": "posts/strategy-pattern-integration/index.html#concrete-schema-implementations",
    "href": "posts/strategy-pattern-integration/index.html#concrete-schema-implementations",
    "title": "Strategies for Numerical Integration",
    "section": "Concrete schema implementations",
    "text": "Concrete schema implementations\n\nRectangle Rule\nIt’s the simplest way of estimating the area under a curve you can think of — cover it with smaller and smaller rectangles with the value of a function at the leftmost point as height constant width.\n\nImplementing this idea is trivial when using numpy, but let’s add some syntactic sugar so the class is sweeter to work with.\n\nimport numpy as np\n\nclass RectangleScheme(IntegrationScheme):\n    \"\"\"Schema for rectangle integration.\"\"\"\n\n    def __init__(\n        self,\n        steps: int,\n    ) -&gt; None:\n        \"\"\"Initializes the rectangle integration config.\"\"\"\n        if steps &lt;= 0:\n            raise ValueError(\"Steps must be greater than 0.\")\n        self._steps = steps\n\n    def __str__(self) -&gt; str:\n        \"\"\"Returns the string representation of the schema.\"\"\"\n        return f\"Rectangle schema with {self._steps} steps\"\n\n    def integrate(\n        self,\n        integrand: Callable[[float], float],\n        *,\n        start: float,\n        end: float,\n    ) -&gt; float:\n        \"\"\"Integrates a function using rectangle integration.\"\"\"\n        x_points = np.linspace(start, end, self._steps)\n        values = integrand(x_points)\n        dx = (end - start) / np.float64(self._steps)\n        return np.sum(values) * dx\n\n\nRectangleScheme subclasses IntegrationScheme so we need to implement the integrate method.\n__init__ method is run each time object of this class is requested. It sets the stage — in this case all we need is the number of rectangles we are to use. To be cautious, we check if the steps number is positive.\n__str__ is called when we try to represent the object as string — ex. in f-strings or directly calling str(). We just taught our class objects to introduce themselves nicely.\nintegrate is as simple as the idea behind it:\n\nget the equaly spaced x values,\ncalculate integrand values at the points,\nsum it up,\nmultiply the sum by the distance between two consecutive points.\n\n\n\n\nSimple Monte Carlo\nThis guy sounds fancy with its luxurious Monaco vibes, but it’s just a peasant in a nice suit. Instead of looking at equaly-spaced points, we shuffle them from uniform distribution on the interval of integration. We calculate the integrand function values at those points and sum them up. Then multiply the sum by the average distance between points and through the magic of probability theory (and not opening actual probability textbook in 10 years) you get a good probabilistic estimator of the integral value. The implementation is analogous to the RectangleScheme.\n\nfrom typing import Optional\n\nclass MonteCarloScheme(IntegrationScheme):\n    \"\"\"Schema for Monte Carlo integration.\"\"\"\n\n    def __init__(\n        self,\n        random_points: int,\n        random_seed: Optional[int] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the rectangle integration config.\"\"\"\n        if random_points &lt;= 0:\n            raise ValueError(\"Points must be greater than 0.\")\n        self.__random_points = random_points\n        self.__random_seed = random_seed\n\n    def __str__(self) -&gt; str:\n        \"\"\"Returns the string representation of the schema.\"\"\"\n        points_msg = f\"Monte Carlo schema with {self.__random_points} random points\"\n        seed_msg = f\" and seed {self.__random_seed}\" if self.__random_seed else \"\"\n        return f\"{points_msg}{seed_msg}\"\n\n    def integrate(\n        self,\n        integrand: Callable[[float], float],\n        *,\n        start: float,\n        end: float,\n    ) -&gt; float:\n        \"\"\"Integrates a function using Monte Carlo integration.\"\"\"\n        np.random.seed(seed=self.__random_seed)\n        x_points = np.random.uniform(start, end, self.__random_points)\n        values = integrand(x_points)\n        average_dx = (end - start) / np.float64(self.__random_points)\n        return np.sum(values) * average_dx\n\n\nOptional[int] annotation means that the value of random_seed can be a float or None. With a set seed we get a reproducable results — good for testing but not for actual usage. Hence the default value here is None.\n\n\n\n\n\n\n\nNote\n\n\n\nThe Optional stands for could be None as well, it doesn’t affect if the input is mandatory or not. In our case it’s not, but thats stated by the = None part. In Python 3.11 onwards it’s recommended to use int | None instead."
  },
  {
    "objectID": "posts/strategy-pattern-integration/index.html#integrator",
    "href": "posts/strategy-pattern-integration/index.html#integrator",
    "title": "Strategies for Numerical Integration",
    "section": "Integrator",
    "text": "Integrator\nWhat’s left is to have a way of defining the problem to solve and define how our schemes (strategies) interact with it.\n\n\"\"\"An integrator class that allows to perform integration using different schemas.\"\"\"\nfrom typing import Callable\n\nclass Integrator:\n    \"\"\"An integrator class that allows to perform integration using different\n    schemas as strategies.\"\"\"\n\n    def __init__(\n        self,\n        integrand: Callable[[float], float],\n        interval_start: float,\n        interval_end: float,\n    ) -&gt; None:\n        \"\"\"Initializes the integrator class.\"\"\"\n        if interval_start &gt;= interval_end:\n            raise ValueError(\"Start value must be less than end value.\")\n        self.__integrand = integrand\n        self.__interval_start = interval_start\n        self.__interval_end = interval_end\n\n    def __call__(\n        self,\n        schema: IntegrationScheme,\n    ) -&gt; float:\n        \"\"\"\n        Calculates the definite integral value of a function.\n\n        Args:\n            schema: integration schema\n        \"\"\"\n        print(f\"Using {schema}.\")\n        return schema.integrate(\n            self.__integrand,\n            start=self.__interval_start,\n            end=self.__interval_end,\n        )\n\n\nThe __init__ takes in the obvious parameters — function to integrate, start and end of the interval. It also checks if it’s a proper integral.\nWe get to implement our own __call__ method now. It’s clear what Integrator class does. No need to have a method with a descriptive name like Integrator.integrate. To use it you pass through the integration scheme into the integrator — notice annotation of the abstract IntegrationScheme. It prints the info on strategy used (using the __str__ methods) and calls integrate method of the scheme. No care in the world on how the value is actually calculated."
  },
  {
    "objectID": "posts/strategy-pattern-integration/index.html#lets-integrate",
    "href": "posts/strategy-pattern-integration/index.html#lets-integrate",
    "title": "Strategies for Numerical Integration",
    "section": "Let’s integrate!",
    "text": "Let’s integrate!\nOk, now to the integrating! Let’s set up the stage:\n\nstart, end = 0, np.pi / 2.0\n\ndef f(x: float) -&gt; float:\n    return np.sin(x) + np.cos(x)\n\nExcited? Don’t be… yet.\nWe should get some benchmark value first. As none of us would bother to integrate this by hand, we’ll use SciPy. Unexpectedly (SciPy uses C and Fortran underneath), we get the result in a breeze and it is very close to actual value of 2.0.\n\nfrom scipy.integrate import quad\n\nscipy_quad, err = quad(f, start, end)\nprint(scipy_quad)\n\n1.9999999999999998\n\n\nNow let’s use our Integrator class and see.\n\nintegrator = Integrator(\n    f,\n    interval_start=start,\n    interval_end=end,\n)\n\niterations = [2**i for i in range(0,21,5)]\nrectangle_results = [integrator(RectangleScheme(steps=i)) for i in iterations]\nmc_results = [integrator(MonteCarloScheme(random_points=i)) for i in iterations]\n\nprint(f\"Rectangle schema results:\\n{rectangle_results}.\")\nprint(f\"Monte Carlo schema results:\\n{mc_results}.\")\n\nUsing Rectangle schema with 1 steps.\nUsing Rectangle schema with 32 steps.\nUsing Rectangle schema with 1024 steps.\nUsing Rectangle schema with 32768 steps.\nUsing Rectangle schema with 1048576 steps.\nUsing Monte Carlo schema with 1 random points.\nUsing Monte Carlo schema with 32 random points.\nUsing Monte Carlo schema with 1024 random points.\nUsing Monte Carlo schema with 32768 random points.\nUsing Monte Carlo schema with 1048576 random points.\nRectangle schema results:\n[np.float64(1.5707963267948966), np.float64(1.986172817555692), np.float64(1.9995804632216618), np.float64(1.9999869013603688), np.float64(1.9999995906791057)].\nMonte Carlo schema results:\n[np.float64(2.1674863935509348), np.float64(2.049404880963646), np.float64(1.9956862021349064), np.float64(2.00035743340837), np.float64(2.0000909818381047)].\n\n\nThe performance and convergence of those schemes is terrible. Like anything in Python, if you want robust and performing code, you need to implement it with C or use any/all of the enhancement frameworks that Python provides (see Numba). Additionally, the simple methods we implemented are very naive. The standard numerical packages use sophisticated algorithms honed for many decades.\nBut I was wrong! You should be excited! We just learned new approach for setting up extensible and readable code! Look how cleanly the problem statement is separated form different strategies to solve it.\nIf you are now wondering how much we could improve by using more advanced techniques (like stratified Monte Carlo or adaptive quadrature) you just need to implement new subclass of `IntegrationSchema’ and you’re done. No changes to the existing code are needed, just simple extension. And that’s the idea behind strategy pattern.\n\n\n\n\n\n\nNote\n\n\n\nDownload the whole code here."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in the bwrob blog. Welcome!\nThis blog is a battleground of sorts, but instead of swords and shields, we wield the weapons of Python and C++. I’m a mathematician turned quantitative analyst turned software engineer. You can expect high standard deviation of topics here.\nHere, I’ll document my coding conquests, from building practical and impractical tools, exploring financial concepts, to playing around with physics simulations.\nExpect a healthy dose of humor alongside the technical discussions. Let’s be honest, even the most complex problems are more enjoyable with a sprinkle of laughter. So, grab a cup of coffee and join me on this exploration – even if it’s just for one interested reader!\n\n\n\n Back to top"
  },
  {
    "objectID": "pages/resume.html",
    "href": "pages/resume.html",
    "title": "Bartosz Wróblewski",
    "section": "",
    "text": "Mathematician at heart, quantitative finance technologist by trade. I leverage diverse experiences in academia, derivatives valuation, market risk, and quantitative development to bring a generalist’s perspective to quantitative finance and software engineering.\n\n \n  \n   \n  \n    \n     LinkedIn\n  \n  \n    \n     Github\n  \n  \n    \n     Contact"
  },
  {
    "objectID": "pages/resume.html#professional-experience",
    "href": "pages/resume.html#professional-experience",
    "title": "Bartosz Wróblewski",
    "section": "Professional Experience",
    "text": "Professional Experience\n\nDXC Luxoft\n\nSenior Software Developer | Sep 2024 – Present\nRisk Engine Engineer role in the counterparty credit risk (CCR) modelling and analytics team within R&C model development group.\n\n\n\nSyberry\n\nFinancial Software Engineer | Sep 2023 – Sep 2024\nDevelopment of a financial platform for a hedge fund client.\n\nQuantitative developer bridging the gap between hedge fund risk managers and software engineers.\nDeveloping risk management platform that pipes and transforms trading data.\nResponsible for integration of vendor risk management system into the platform, efficient orchestration of asynchronous API calls and resources.\nWorking directly with client’s risk manager and CRO on specifying the business needs and requirements.\n\nTechnologies\n\nBackend: Python, Dagster, FastAPI, Pytest, Pandas, NumPy, SQLAlchemy.\nDatabases: PostgreSQL, DuckDB.\nInfrastructure/pipelines: Amazon Web Services, Terraform, Terraspace, Docker, GitHub Actions.\n\n\n\n\nBank of New York Mellon\n\nSenior Specialist, Model Development | Jul 2022 – Sep 2023\nIndependent specialist at Risk and Compliance department (MO). Market Risk team responsible for modeling of VaR and SVaR, comprehensive test scenarios and portfolio sensitivity exposures.\n\nDeveloping and maintaining market risk models — VaR, SVaR — and stress testing frameworks implemented in C++.\nCalibrating and benchmarking vendor pricing models (Murex risk management system).\nDesigning, creating and analyzing risk-related reports and ad hoc analyses for front office and risk management.\nResponsible for Market Risk RWA projections submission for CCAR 2023.\nCoordinating development projects with IT, Validation and Risk Management.\nDeveloped a method for directly comparing interest rate sensitivities between FO and MO systems using different conventions and curves setups.\nRe-implemented legacy C++ volatility surface builder in Python without performance loss.\n\nTechnologies\n\nInternal risk models: C++.\nVendor pricing system: Murex.\nDatabases: MS SQL.\nTooling: Python, Quantlib, VBA.\n\n\n\n\nCredit Suisse\n\nQuantitative Analyst | Nov 2018 – Jun 2022\nJunior analyst at Credit Derivatives division, part of global front office QuantStrats department (700+ quants, developers and data analysts).\n\nDevelopment and maintenance of an in-house valuation framework used across bank’s systems (COM C++ and .Net). Contributions to valuation models and market data object builders used in pricing.\nProviding direct support to traders and Middle Office in ad hoc investigations on valuation, risks and technical issues.\nExtending and creating COM-addin-based Excel pricing sheets used by Trading and Product Control.\nAssisting with calculation and analysis of risk profiles, PnL reports, transition impact assessments of trade portfolios.\nRepresented the company by giving lectures and taking part in campus recruitment programs.\nCoordinated resolution of strategic process of calculating Collateral Adjusted Valuation for the xVA desk.\n\nStructured Notes\nCredit Suisse issued structured note products (corporate bonds with derivatives attached to payout).\n\nContributed to structured notes valuation model framework implemented in factorized C++.\nUpdated credit curve models and utilities to be compatible with OIS discounting in preparation to LIBOR cessation (F#).\nExtended trading pricing sheets with logic needed to price instruments with compounding rates financing (Excel, COM-addins, .Net extensions).\nCollaborated with quantitative developers from in-house risk management system project on addition of new market data builders (C#).\n\nLongevity Derivatives\nLongevity-based (insurance policies and pension schemes) financial derivatives.\n\nMigrated existing legacy tools to 64-bit environment.\nProvided technical support to Product Control in their monthly tie-in processes.\n\nTechnologies\n\nPricing framework: F#, C++, COM.\nRisk Managment: C#.\nTrading tools: Excel, VBA.\nCI/CD: Perforce, TeamCity."
  },
  {
    "objectID": "pages/resume.html#academic-experience",
    "href": "pages/resume.html#academic-experience",
    "title": "Bartosz Wróblewski",
    "section": "Academic Experience",
    "text": "Academic Experience\n\nWrocław University of Science and Technology\n\nTeaching Assistant | Oct 2024 – Present\nTeaching computer science for Mathematics students.\n\n\n\nUniversity of Wrocław\n\nPhD Candidate | Oct 2016 – Jan 2019\nResearched evolution equations involving non-local unbounded operators. Focused on applications of functional and harmonic analysis. Main ‘hero’ of my research was the fractional Laplacian operator — jump-diffusion analogue of the classical Laplacian.\nAlso interested in hydrodynamic models and applications of PDEs in physics (porous medium modelling, flocking models, quantum mechanics).\nUnfinished.\n\n\nJunior Researcher | Apr 2017 – Jan 2019\nParticipated in the Polish National Science Center research grant Nonlocal parabolic problems: regularity, blowup, pattern formation. Principal Investigator Prof. Piotr Biler.\n\n\nTeaching Assistant | Mar 2017 – Jul 2018\nNon-linear Functional Analysis, Ordinary Differential Equations, Honors Ordinary Differential Equations.\n\n\n\nUniversity of Warsaw\n\nResearch Intern | Dec 2016 – Mar 2017\nVisiting position during CrossFields PDEs semester organised and sponsored by The Simons Foundation.\nCollaboration with Raphael Dunchin, Piotr B. Mucha and with Jan Peszek in the research on fractional Euler alignment system (hydrodynamic flocking dynamics).\n\nRegular solutions to the fractional Euler alignment system in the Besov spaces framework published in Mathematical Models and Methods in Applied Sciences Vol. 29, No. 01, pp. 89-119\n\n\n\nTeaching Assistant | Jan 2017 – Mar 2017\nAnalysis 1, Analysis 2."
  },
  {
    "objectID": "pages/resume.html#education",
    "href": "pages/resume.html#education",
    "title": "Bartosz Wróblewski",
    "section": "Education",
    "text": "Education\n\nUniversity of Wrocław\n\nMsc in Theoretical Mathematics | Oct 2014 – Sep 2016\nThesis topic: “The anomalous diffusion and fractional Laplacian on the half-line” written under the supervision of Prof. Grzegorz Karch.\n\n\n\nWrocław University of Science and Technology\n\nBSc in Theoretical Mathematics | Oct 2011 – Jul 2014"
  },
  {
    "objectID": "pages/notebooks/cd_projekt_red.html",
    "href": "pages/notebooks/cd_projekt_red.html",
    "title": "bwrob blog",
    "section": "",
    "text": "import mplfinance as mpf\nimport yfinance as yf\nfrom rich import print as rprint\n\n\nticker_name = \"CDR.WA\"\nyf_ticker = yf.Ticker(ticker_name)\nrprint(yf_ticker.info)\n\n{\n    'address1': 'Building E',\n    'address2': 'ul. Jagiellonska 74',\n    'city': 'Warsaw',\n    'zip': '03-301',\n    'country': 'Poland',\n    'phone': '48 22 519 6900',\n    'fax': '48 22 375 7710',\n    'website': 'https://www.cdprojekt.com',\n    'industry': 'Electronic Gaming & Multimedia',\n    'industryKey': 'electronic-gaming-multimedia',\n    'industryDisp': 'Electronic Gaming & Multimedia',\n    'sector': 'Communication Services',\n    'sectorKey': 'communication-services',\n    'sectorDisp': 'Communication Services',\n    'longBusinessSummary': 'CD Projekt S.A., together its subsidiaries, engages in the development, publishing, and\ndigital distribution of video games for personal computers and video game consoles in Poland. The company operates \nthrough two segments, CD PROJEKT RED and GOG.com. Its product portfolio comprises The Witcher; The Witcher 2: \nAssassins of Kings; The Witcher 3: Wild Hunt; Thronebreaker: The Witcher Tales; Gwent: The Witcher Card game; and \nCyberpunk 2077. The company also distributes videogames through GOG.com distribution platform and the GOG GALAXY \napplication, as well as online channels. In addition, it exports its products in Europe, North America, South \nAmerica, Asia, Australia, and Africa. CD Projekt S.A. was incorporated in 2001 and is headquartered in Warsaw, \nPoland.',\n    'fullTimeEmployees': 1134,\n    'companyOfficers': [\n        {\n            'maxAge': 1,\n            'name': 'Mr. Marcin Piotr Iwinski',\n            'age': 49,\n            'title': 'Co-Founder & Chairman of Supervisory Board',\n            'yearBorn': 1974,\n            'exercisedValue': 0,\n            'unexercisedValue': 0\n        },\n        {\n            'maxAge': 1,\n            'name': 'Mr. Adam Konrad Badowski',\n            'age': 48,\n            'title': 'Co-CEO & Member of the Management Board',\n            'yearBorn': 1975,\n            'fiscalYear': 2023,\n            'totalPay': 5633000,\n            'exercisedValue': 0,\n            'unexercisedValue': 0\n        },\n        {\n            'maxAge': 1,\n            'name': 'Mr. Michal Andrzej Nowakowski',\n            'age': 45,\n            'title': 'Co-CEO & Member of the Management Board',\n            'yearBorn': 1978,\n            'fiscalYear': 2023,\n            'totalPay': 5633000,\n            'exercisedValue': 0,\n            'unexercisedValue': 0\n        },\n        {\n            'maxAge': 1,\n            'name': 'Mr. Piotr Marcin Nielubowicz',\n            'age': 49,\n            'title': 'CFO & Vice President of the Management Board',\n            'yearBorn': 1974,\n            'fiscalYear': 2023,\n            'totalPay': 5633000,\n            'exercisedValue': 0,\n            'unexercisedValue': 0\n        },\n        {\n            'maxAge': 1,\n            'name': 'Mr. Adam Michal Kicinski',\n            'age': 54,\n            'title': 'Chief Strategy Officer & Member of the Management Board',\n            'yearBorn': 1969,\n            'fiscalYear': 2023,\n            'totalPay': 5633000,\n            'exercisedValue': 0,\n            'unexercisedValue': 0\n        },\n        {\n            'maxAge': 1,\n            'name': 'Mr. Piotr  Karwowski',\n            'age': 41,\n            'title': 'Joint COO & Member of the Management Board',\n            'yearBorn': 1982,\n            'fiscalYear': 2023,\n            'totalPay': 5573000,\n            'exercisedValue': 0,\n            'unexercisedValue': 0\n        },\n        {\n            'maxAge': 1,\n            'name': 'Mr. Pawel  Zawodny',\n            'age': 46,\n            'title': 'Joint COO & Member of the Management Board',\n            'yearBorn': 1977,\n            'fiscalYear': 2023,\n            'totalPay': 3116000,\n            'exercisedValue': 0,\n            'unexercisedValue': 0\n        },\n        {\n            'maxAge': 1,\n            'name': 'Mr. Jeremiah  Cohn',\n            'age': 41,\n            'title': 'Chief Marketing Officer & Member of Management Board',\n            'yearBorn': 1982,\n            'fiscalYear': 2023,\n            'totalPay': 2988008,\n            'exercisedValue': 0,\n            'unexercisedValue': 0\n        },\n        {\n            'maxAge': 1,\n            'name': 'Mr. Maciej  Nielubowicz',\n            'age': 38,\n            'title': 'Secretary of the Supervisory Board & Member of Supervisory Board',\n            'yearBorn': 1985,\n            'fiscalYear': 2023,\n            'exercisedValue': 0,\n            'unexercisedValue': 0\n        },\n        {\n            'maxAge': 1,\n            'name': 'Krystyna  Cybulska',\n            'title': 'Chief Accountant',\n            'fiscalYear': 2023,\n            'exercisedValue': 0,\n            'unexercisedValue': 0\n        }\n    ],\n    'compensationAsOfEpochDate': 1703980800,\n    'maxAge': 86400,\n    'priceHint': 2,\n    'previousClose': 174.25,\n    'open': 173.15,\n    'dayLow': 173.15,\n    'dayHigh': 177.3,\n    'regularMarketPreviousClose': 174.25,\n    'regularMarketOpen': 173.15,\n    'regularMarketDayLow': 173.15,\n    'regularMarketDayHigh': 177.3,\n    'dividendRate': 1.0,\n    'dividendYield': 0.0057,\n    'exDividendDate': 1718841600,\n    'payoutRatio': 0.17860001,\n    'beta': 0.352,\n    'trailingPE': 31.042778,\n    'forwardPE': 124.39285,\n    'volume': 341530,\n    'regularMarketVolume': 341530,\n    'averageVolume': 385098,\n    'averageVolume10days': 354646,\n    'averageDailyVolume10Day': 354646,\n    'bid': 173.9,\n    'ask': 174.15,\n    'marketCap': 17399412736,\n    'fiftyTwoWeekLow': 97.1,\n    'fiftyTwoWeekHigh': 187.0,\n    'priceToSalesTrailing12Months': 13.084262,\n    'fiftyDayAverage': 169.687,\n    'twoHundredDayAverage': 134.283,\n    'trailingAnnualDividendRate': 1.0,\n    'trailingAnnualDividendYield': 0.005738881,\n    'currency': 'PLN',\n    'enterpriseValue': 16931279872,\n    'profitMargins': 0.42179,\n    'floatShares': 89921457,\n    'sharesOutstanding': 99910496,\n    'heldPercentInsiders': 0.29748,\n    'heldPercentInstitutions': 0.20241,\n    'impliedSharesOutstanding': 99910496,\n    'bookValue': 24.866,\n    'priceToBook': 7.003539,\n    'lastFiscalYearEnd': 1703980800,\n    'nextFiscalYearEnd': 1735603200,\n    'mostRecentQuarter': 1719705600,\n    'earningsQuarterlyGrowth': 2.36,\n    'netIncomeToCommon': 560892032,\n    'trailingEps': 5.61,\n    'forwardEps': 1.4,\n    'pegRatio': 1.78,\n    'enterpriseToRevenue': 12.732,\n    'enterpriseToEbitda': 31.777,\n    '52WeekChange': 0.34895432,\n    'SandP52WeekChange': 0.36246562,\n    'lastDividendValue': 1.0,\n    'lastDividendDate': 1718841600,\n    'exchange': 'WSE',\n    'quoteType': 'EQUITY',\n    'symbol': 'CDR.WA',\n    'underlyingSymbol': 'CDR.WA',\n    'shortName': 'CDPROJEKT',\n    'longName': 'CD Projekt S.A.',\n    'firstTradeDateEpochUtc': 946886400,\n    'timeZoneFullName': 'Europe/Warsaw',\n    'timeZoneShortName': 'CEST',\n    'uuid': '1f0ca8ab-1b56-3fda-b9c6-22298a0a5121',\n    'messageBoardId': 'finmb_32503885',\n    'gmtOffSetMilliseconds': 7200000,\n    'currentPrice': 174.15,\n    'targetHighPrice': 170.0,\n    'targetLowPrice': 80.0,\n    'targetMeanPrice': 112.18,\n    'targetMedianPrice': 120.0,\n    'recommendationMean': 3.5,\n    'recommendationKey': 'hold',\n    'numberOfAnalystOpinions': 17,\n    'totalCash': 489824992,\n    'totalCashPerShare': 4.903,\n    'ebitda': 532811008,\n    'totalDebt': 21690000,\n    'quickRatio': 4.631,\n    'currentRatio': 8.12,\n    'totalRevenue': 1329796992,\n    'debtToEquity': 0.873,\n    'revenuePerShare': 13.31,\n    'returnOnAssets': 0.13365,\n    'returnOnEquity': 0.24827999,\n    'freeCashflow': -133298000,\n    'operatingCashflow': 740211008,\n    'earningsGrowth': 2.378,\n    'revenueGrowth': 0.316,\n    'grossMargins': 0.69961,\n    'ebitdaMargins': 0.40067002,\n    'operatingMargins': 0.29679,\n    'financialCurrency': 'PLN',\n    'trailingPegRatio': None\n}\n\n\n\n\nperiod = \"6mo\"\ninterval = \"1d\"\nhistorical_data = yf_ticker.history(\n    period=period,\n    interval=interval,\n)\nhistorical_data.tail(10)\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nVolume\nDividends\nStock Splits\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n2024-09-17 00:00:00+02:00\n173.750000\n177.000000\n173.399994\n176.399994\n232807\n0.0\n0.0\n\n\n2024-09-18 00:00:00+02:00\n177.000000\n177.800003\n172.500000\n176.350006\n216791\n0.0\n0.0\n\n\n2024-09-19 00:00:00+02:00\n176.449997\n180.100006\n175.449997\n177.800003\n287095\n0.0\n0.0\n\n\n2024-09-20 00:00:00+02:00\n176.199997\n177.800003\n173.550003\n175.699997\n897195\n0.0\n0.0\n\n\n2024-09-23 00:00:00+02:00\n175.699997\n175.699997\n165.250000\n167.600006\n528469\n0.0\n0.0\n\n\n2024-09-24 00:00:00+02:00\n168.000000\n171.949997\n166.250000\n171.550003\n288599\n0.0\n0.0\n\n\n2024-09-25 00:00:00+02:00\n171.500000\n172.300003\n169.000000\n172.100006\n296240\n0.0\n0.0\n\n\n2024-09-26 00:00:00+02:00\n172.600006\n175.750000\n171.000000\n171.850006\n295868\n0.0\n0.0\n\n\n2024-09-27 00:00:00+02:00\n170.000000\n175.000000\n169.500000\n174.250000\n161866\n0.0\n0.0\n\n\n2024-09-30 00:00:00+02:00\n173.149994\n177.300003\n173.149994\n174.149994\n341530\n0.0\n0.0\n\n\n\n\n\n\n\n\nmpf.plot(\n    historical_data,\n    type=\"candle\",\n    mav=(10, 24),\n    volume=True,\n    style=\"yahoo\",\n    figratio=(3, 2),\n    figscale=1.5,\n    show_nontrading=False,\n    title=yf_ticker.info[\"longName\"],\n    tight_layout=True,\n    warn_too_much_data=10_000,\n)\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "pages/hobbies.html",
    "href": "pages/hobbies.html",
    "title": "Fueling the Mind & Body: My Off-Duty Explorations.",
    "section": "",
    "text": "Supprisingly, I still have some hobbies outside technology and finance. This list is meant to be aspirational – i.e. sort of Instagram take on my interests to motivate me to invest more time into things I like. Too often I find myself in a work-sleep-work cycle that leaves little time for enjoyment.\n\n\nI have a PS5, a NS and a gaming PC. I play only single-player games, enjoy rouge-likes and open-worlds the most. My favorite games are God of War (2018), Hades and Trackmania. If your life’s companion is also a gamer (or just plays a little bit) i highly recommend It Takes Two.\n\n\n\n\nGot two cameras, old big hog Canon 450D and compact and sleek Sony A6000. I can make a decent portrait or Instagram ‘location photo’ but mosty interested in nature and urban settings. Would like to learn more on light and exposure usage, for now I am looking for contrasts and interesting compositions in my photos.\n\n\n\n\nClever ideas that fill and fuel natural sciences are often intimidating. Sure, there are occasional outliers – genius individuals that make history with revolutionary inventions. But most of the progress is achieved by a tectonic creep of incremental small wins and observations. All of those geniuses stood on the shoulders of giants that came before. The history of how those ideas and meanders of knowledge happened can be thrilling and educating.\n\n\n\n\nMaster chef of making tasty meal from leftovers. Specializing in Asian fusion cuisine. Fried rice with pulled tofu? Sushi with beetroot? Korean beef with sauerkraut? Yes, those are all delicious!\n\n\n\n\nI am a proud owner of 90’s vintage road bike. It was recently refurbished and looks smashing! It even has chipset print and binary code on the handles (photo pre-renovation)!\n\n\n\n\nYes, just strolling, but with a philosophy. No headphones, no distractions, no direction. Just observing what changed in the neighborhood I’ve lived in for the last 15 years, and what is happening inside. Think awareness meditation but while walking."
  },
  {
    "objectID": "pages/hobbies.html#road-cycling",
    "href": "pages/hobbies.html#road-cycling",
    "title": "Fueling the Mind & Body: My Off-Duty Explorations.",
    "section": "",
    "text": "I am a proud owner of 90’s vintage road bike. It was recently refurbished and looks smashing! It even has chipset print and binary code on the handles (photo pre-renovation)!"
  },
  {
    "objectID": "pages/hobbies.html#strolling.",
    "href": "pages/hobbies.html#strolling.",
    "title": "Fueling the Mind & Body: My Off-Duty Explorations.",
    "section": "",
    "text": "Yes, just strolling, but with a philosophy. No headphones, no distractions, no direction. Just observing what changed in the neighborhood I’ve lived in for the last 15 years, and what is happening inside. Think awareness meditation but while walking."
  },
  {
    "objectID": "pages/courses/pwr-intro-to-cs-2024.html",
    "href": "pages/courses/pwr-intro-to-cs-2024.html",
    "title": "Wstęp do programowania (Zima 2024/2025)",
    "section": "",
    "text": "Zasady zaliczenia na stronie kursu u wykładowcy.\nDodatkowe zasady moich grup lablatorialnych:\n\nListy zadań - co najmniej pierwsze 4 listy będziemy robić na zajęciach, w systemie deklaracyjnym. Po zajęciach proszę o przesłanie rozwiązań. Kolejne listy zadań będą w formie zadań domowych, do zrobienia i przesłania przed zajęciami. Rozwiązania będą omiawiane i wyrywkowo sprawdzane w trakcie zajęć.\nOdpowiedź ustna - od 7-mych zajęć można zgłaszać się na ochotnika, co najmniej 3 ochotników jest wymaganych na jednych zajęciach.\nProjekt grupowy - można się łączyć w grupy w obrębie moich 2 grup, ale nie poza. Zakładam 5-8 grup.\nAktywność - punkty zostaną przydzielone na zasadzie normalizacji aktywności na zajęciach. Gwarantuję, że jedna duża aktywność (np. przedstawienie pełnego rozwiązania z listy) będzie dawała co najmniej jeden punkt."
  },
  {
    "objectID": "pages/courses/pwr-intro-to-cs-2024.html#informacje-ogólne",
    "href": "pages/courses/pwr-intro-to-cs-2024.html#informacje-ogólne",
    "title": "Wstęp do programowania (Zima 2024/2025)",
    "section": "",
    "text": "Zasady zaliczenia na stronie kursu u wykładowcy.\nDodatkowe zasady moich grup lablatorialnych:\n\nListy zadań - co najmniej pierwsze 4 listy będziemy robić na zajęciach, w systemie deklaracyjnym. Po zajęciach proszę o przesłanie rozwiązań. Kolejne listy zadań będą w formie zadań domowych, do zrobienia i przesłania przed zajęciami. Rozwiązania będą omiawiane i wyrywkowo sprawdzane w trakcie zajęć.\nOdpowiedź ustna - od 7-mych zajęć można zgłaszać się na ochotnika, co najmniej 3 ochotników jest wymaganych na jednych zajęciach.\nProjekt grupowy - można się łączyć w grupy w obrębie moich 2 grup, ale nie poza. Zakładam 5-8 grup.\nAktywność - punkty zostaną przydzielone na zasadzie normalizacji aktywności na zajęciach. Gwarantuję, że jedna duża aktywność (np. przedstawienie pełnego rozwiązania z listy) będzie dawała co najmniej jeden punkt."
  },
  {
    "objectID": "pages/courses/pwr-intro-to-cs-2024.html#notki-z-zajęć",
    "href": "pages/courses/pwr-intro-to-cs-2024.html#notki-z-zajęć",
    "title": "Wstęp do programowania (Zima 2024/2025)",
    "section": "Notki z zajęć",
    "text": "Notki z zajęć\n\nRozważania wstępne\n\nPo co matematykowi komputer?\nCo i dlaczego będzie na kursie?\nOrganizacja pracy i zasady zaliczenia.\nCzym jest język programowania?\nCharakterystyka języka Python.\nŚrodowisko programistyczne Jupyter Notebook.\nPodstawy języka znacznikowego Markdown.\n\n\nPrzykłady zeszytów Jupyter\nPrzykłady ilustrujące wachlarz możliwości zeszytów Jupyter i Markdown w zastosowaniach matematycznych. Wymagają one osobnych ustawień środowiska programistycznego i zewnętrznych narzędzi. Polecam zapoznanie się z nimi czysto ilustracyjnie jako motywacja do nauki.\n\nWizualizacja zbioru Mandelbrota obliczana na karcie graficznej - link\nPobieranie i analiza danych o cenie akcji spółki giełdowej - link\nNumeryczne rozwiązanie równań różniczkowych cząstkowych - link\nRozwiązania zadań z podstaw algebry liniowej - link\nCała obecna strona została utworzona z wykorzystaniem Markdownu i Pythona korzystając z technologii Quarto - link\n\n\n\nPython lista 1\nlink\n\nDo zadania 6 jeszcze wrócimy po funkcjach i pętlach aby zaimplemetować dużo czytelniejsze rozwiązanie."
  },
  {
    "objectID": "pages/teaching.html",
    "href": "pages/teaching.html",
    "title": "Dydaktyka",
    "section": "",
    "text": "Wstęp do programowania"
  },
  {
    "objectID": "pages/teaching.html#politechnika-wrocławska",
    "href": "pages/teaching.html#politechnika-wrocławska",
    "title": "Dydaktyka",
    "section": "",
    "text": "Wstęp do programowania"
  },
  {
    "objectID": "pages/teaching.html#uniwersytet-wrocławski",
    "href": "pages/teaching.html#uniwersytet-wrocławski",
    "title": "Dydaktyka",
    "section": "Uniwersytet Wrocławski",
    "text": "Uniwersytet Wrocławski"
  },
  {
    "objectID": "pages/teaching.html#uniwersytet-warszawski",
    "href": "pages/teaching.html#uniwersytet-warszawski",
    "title": "Dydaktyka",
    "section": "Uniwersytet Warszawski",
    "text": "Uniwersytet Warszawski"
  },
  {
    "objectID": "pages/notebooks/mandebrot.html",
    "href": "pages/notebooks/mandebrot.html",
    "title": "bwrob blog",
    "section": "",
    "text": "from functools import partial\n\nfrom mandelbrot import (\n    create_fractal,\n    create_fractal_gpu,\n    create_fractal_jit,\n    show_fractal,\n)\n\n\nresolution = (1440, 3440)\niter_small = 10\niter_medium = 10_000\niter_large = 1_000_000\nview_rectangle = (-2.5, 1.5, -1.2, 1.2)\n\n\ndef show_size(iters: int):\n    return partial(\n        show_fractal,\n        max_iters=iters,\n        resolution=resolution,\n        view=view_rectangle,\n    )\n\n\nshow_quick = show_size(iter_small)\nshow_medium = show_size(iter_medium)\nshow_long = show_size(iter_large)\n\n\nfor worker in [\n    create_fractal,\n    create_fractal_jit,\n    create_fractal_gpu,\n]:\n    show_quick(worker)\n\nMandelbrot created in 4.110897000180557s\nMax iterations to escape 10\n\n\n\n\n\n\n\n\n\nMandelbrot created in 0.33227419992908835s\nMax iterations to escape 10\n\n\n\n\n\n\n\n\n\nMandelbrot created in 0.35070399986580014s\nMax iterations to escape 10\n\n\n\n\n\n\n\n\n\n\nfor worker in [\n    create_fractal_jit,\n    create_fractal_gpu,\n]:\n    show_medium(worker)\n\nMandelbrot created in 16.01580259995535s\nMax iterations to escape 10000\n\n\n\n\n\n\n\n\n\nMandelbrot created in 0.5051736000459641s\nMax iterations to escape 10000\n\n\n\n\n\n\n\n\n\n\nshow_fractal(\n    fractal_worker=create_fractal_gpu,\n    max_iters=10_000_000,\n    resolution=(14400, 34400),\n    view=view_rectangle,\n)\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/startup-script/index.html#why-a-startup-script",
    "href": "posts/startup-script/index.html#why-a-startup-script",
    "title": "Python Startup Script",
    "section": "Why a startup script?",
    "text": "Why a startup script?\nConfession time. Pre-COVID I worked for a Swiss bank — it was the whole corporate package. Working from a crowded office, open space, dedicated desks, dedicated desktop PCs, hundreds of people. And we never turned the PCs off, some of them were running constantly for months. Not very eco-friendly behaviour, either from the company side or from the employees. But there were legit reasons for this and the company had much bigger sins on their books.\nAs for the reasons for constantly turned on machines — the proprietary frameworks and tooling we used were awfully heavy and slow. If you were occasionally forced to kill all the processes — due to an update or system crash — getting them all up again took an hour or more. Developer tools, pricing systems, connections to Monte Carlo farms, downloading trade data, analysis definitions — you name it, we used it.\nOne tidbit to get this point across — in 2019 we still used 32-bit Win7, which had a cap on single-process memory consumption to 4GB. We had a universally used hack to extend this to 8GB for Excel, since the default wasn’t enough for many of our pricing sheets.\nWhen I parted ways with the company post-COVID, I got a corporate laptop from the new employer. The office was much sparser, quieter and with ‘hot seats’ approach. I worked mostly remotely either way. This meant switching off the company machine each day, as the VPN connections and certificates wouldn’t last overnight. I was extremely annoyed with opening all of the daily tools again and again, after getting used to having it all waiting for me to jump right in.\nHence the need for a startup script to open all of the apps, tools and files programatically. It won’t cover everything, like authorizations or connections but still a little helper to start the day.\nTo spice things up, we will overengineer the hell out of it and use generic typing, generators and decorators :D."
  },
  {
    "objectID": "posts/startup-script/index.html#the-startup-script",
    "href": "posts/startup-script/index.html#the-startup-script",
    "title": "Python Startup Script",
    "section": "The startup script",
    "text": "The startup script\n\nPublic interface\nLet’s first think about the design and what we want to achieve. Imagine the code being split into two parts:\n\nthe hidden logic layer, the worker that does stuff and\nthe public interface that integrates with the rest of the codebase or is called at the top level, the manager.\n\nWe can start by creating the public interface, see what we want to achieve and later deal with filling in a working implementation. You can either define the logic functions as mocks, or just live with linting issues. This is what my desired usage is:\n\ndef run_startup_script() -&gt; None:\n    \"\"\"Run the startup script.\"\"\"\n    logger.info(f\" Welcome {os.getlogin()}! \".center(40, \"*\"))\n    start_programs(\n        [\n            Program.POWERSHELL,\n            Program.NOTEPAD,\n            (Program.FIREFOX, 8),\n        ],\n    )\n    start_work_files([Path.home() / \".temp\"])\n    run_commands(['Write-Output \"test\"'])\n\nWe specify programs to be launched, open all files in a chosen temp folder and run a list of shell commands. After each step there should be some delay to avoid spamming system with process calls. Certain steps might be more time consuming, like cloud services authorization. Those need longer delay. On the other hand, specifying delay for each task would be cumbersome, most cases would be fine with some default value. We end up with a design that our soon-to-be workers start_programs, start_work_files, and run_commands expect lists of either task or (task, delay_seconds). We can now start the implementation by figuring out how the delay time should be defaulted if not specified.\n\n\nDecorator defaulting the time delays\nNow we can start the implementation. We want the workers to be called with a list of tasks, but a task can be either a command or a tuple including the delay. Let’s not include some complicated conditionals in each of the workers — that would be annoying and potentially get out of sync at some point.\nOther way to do this is to add a default delay to all non-tuple items. We can implement that as a separate defaulting function, which we would call in each worker. But — being more clever! — we could also use a decorator that changes the workers’s signature.\n\nfrom functools import wraps\nfrom collections.abc import Callable\nfrom typing import TypeVar\n\nT = TypeVar(\"T\")\nTask = tuple[T, int]\nTaskList = list[Task[T]]\nTaskListOptionalDelay = list[Task[T] | T]\n\n\nDEFAULT_DELAY_SECONDS = 4\n\ndef with_optional_delay(\n    task_worker: Callable[[TaskList[T]], None],\n) -&gt; Callable[[TaskListOptionalDelay[T]], None]:\n    \"\"\"Add default delay to all non-tuple items.\n\n    Args:\n    ----\n        task_worker: A function that takes a list of tasks.\n\n    \"\"\"\n\n    @wraps(task_worker)\n    def task_defaulted_worker(task_list: TaskListOptionalDelay[T]) -&gt; None:\n        \"\"\"Add a default delay to tasks in a task list if no delay is specified.\n\n        Args:\n        ----\n            task_list: A list of tasks with optional delays.\n\n        \"\"\"\n        tasks_with_defaulted_delays: TaskList[T] = [\n            item if isinstance(item, tuple) else (item, DEFAULT_DELAY_SECONDS)\n            for item in task_list\n        ]\n        return task_worker(tasks_with_defaulted_delays)\n\n    return task_defaulted_worker\n\nFirst, for convenience, let’s define a template type annotation:\n\nThe generic variable type is conventionally denoted by T.\nA Task is a tuple of a generic task T and an int delay.\nA TaskList is a list of Tasks. Simple.\nA TaskListOptionalDelay is a list of Tasks or Ts. Those are the guys we are going to turn into TaskLists.\n\nThe decorator with_optional_delay adds a default delay of 4 seconds to all non-tuple items. Its input is a worker function that already expects a TaskList. It fills in the missing delay with the default value and passes it to the decorated function.\n\n\nMain workers\nWith the preparation done, we can start with the workers. For the programs we will be launching, we need to look up the exact paths of the executables. Let’s define a Program enum that would wrap those up in readable nice names. For working with system paths we will use pathlib.Path that provides high-level interface.\n\nfrom enum import Enum\nfrom pathlib import Path\n\n\nclass Program(Enum):\n    \"\"\"Types of programs.\"\"\"\n\n    POWERSHELL = Path(r\"C:\\windows\\system32\\windowspowershell\\v1.0\\powershell.exe\")\n    NOTEPAD = Path(r\"C:\\Program Files\\Notepad++\\notepad++.exe\")\n    FIREFOX = Path(r\"C:\\ProgramData\\Microsoft\\Windows\\Start Menu\\Programs\\Firefox.lnk\")\n\nOther than that, the main workers start_programs, start_work_files, and run_commands are simple. They just iterate over the provided TaskList and do the respective action for a given Task.\n\n@with_optional_delay\ndef start_programs(\n    programs: TaskList[Program],\n) -&gt; None:\n    \"\"\"Start listed programs.\n\n    Args:\n    ----\n        programs: List of programs to start.\n            Can be a string or a tuple. If a tuple is given, the first\n            element is the name, the second is the delay.\n\n    \"\"\"\n    for program, delay in programs:\n        start_process(\n            name=program.name,\n            path=program.value,\n            delay=delay,\n        )\n\n\n@with_optional_delay\ndef start_work_files(\n    directory_tasks: TaskList[Path],\n) -&gt; None:\n    \"\"\"Start all  files in the work folders.\n\n    Args:\n    ----\n        directory_tasks: List of tasks with paths to work folders.\n\n    \"\"\"\n    for path, delay in path_files(directory_tasks):\n        start_process(\n            name=path.name,\n            path=path,\n            delay=delay,\n        )\n\n\n@with_optional_delay\ndef run_commands(\n    commands: TaskList[str],\n) -&gt; None:\n    \"\"\"Run all commands in the command list.\n\n    Args:\n    ----\n        commands: List of commands to run.\n\n    \"\"\"\n    for command, delay in commands:\n        run_command(command, delay)\n\n\n\nList all files in list of directories\nNotice the use of path_files generator above. When working with complex iterations, it’s a good practice to create a generator that wraps the iteration details. It is beneficial for readability and maintainability, especially in cases where you need to:\n\nyield multiple values at once (ex. yield path, delay for path in directory),\nhave nested iterators (ex. for directory in list for path in directory),\nfilter the iteration (ex. for path in directory if filter(path)).\n\nHere we’ll have all three of them combined!\n\nNAME_EXCLUDES = (\"$\", \"tmp\")\nEXT_EXCLUDES = (\"exe\",)\n\n\ndef filter_excluded(\n    path: Path,\n) -&gt; bool:\n    \"\"\"Filter path based on name and extension exclude lists.\n\n    Args:\n    ----\n        path: Path to filter.\n\n    \"\"\"\n    return (path.stem not in NAME_EXCLUDES) and (path.suffix not in EXT_EXCLUDES)\n\n\n\ndef path_files(\n    directory_tasks: TaskList[Path],\n) -&gt; Generator[tuple[Path, int], None, None]:\n    \"\"\"Generate all files in the work folder that are not excluded.\n\n    Yields the folder path at beginning of the generator.\n\n    Args:\n    ----\n        directory_tasks: List of tasks with paths to work folders.\n\n    \"\"\"\n    for folder, delay in directory_tasks:\n        yield folder, delay\n\n        all_files = folder.glob(FILES_IN_TREE_PATTERN)\n        filtered = filter(filter_excluded, all_files)\n\n        for file in filtered:\n            yield file, delay\n\nThe filter_excluded function filters out files based on exclusions in name and extension lists. Path.stem and Path.suffix are used to check if the file name or extension is in the lists respectively.\n\n\nInteracting with the OS\nWith all of the framework prepared, the next step is to interact with the OS. To open a file, folder or run an app, we can use the os.startfile function. All the rest of the start_process function is just utility logging. For running a shell command we can use the subprocess module, calling powershell with the powershell.exe and adding the command to run.\n\nimport os\nimport subprocess\nimport time\n\n\ndef start_process(\n    *,\n    name: str,\n    path: Path,\n    delay: int = DEFAULT_DELAY_SECONDS,\n) -&gt; None:\n    \"\"\"Given a path, starts the target.\n\n    Behavior:\n        * Minimizes all windows.\n        * Depending on the path target:\n            * executable files are run,\n            * content files are opened with system default program,\n            * folders are opened with system explorer.\n\n    Args:\n    ----\n        name: Name of the process to start.\n        path: Path to the target.\n        delay: Time to wait after starting the process.\n\n    \"\"\"\n    if path.is_dir():\n        logger.info(\"Opening folder %s\", name)\n\n    if path.suffix in (\".exe\", \".lnk\"):\n        logger.info(\"Running app %s\", name)\n    else:\n        logger.info(\"Opening file %s\", name)\n\n    os.startfile(path)\n    time.sleep(delay)\n\n\ndef run_command(\n    command: str,\n    delay: int,\n) -&gt; None:\n    \"\"\"Run a powershell command.\n\n    Args:\n    ----\n        command: Command to run.\n        delay: Time to wait after starting the process.\n\n    \"\"\"\n    _ = subprocess.call(\n        f\"powershell.exe {command}\",\n        shell=False,\n    )\n    time.sleep(delay)"
  },
  {
    "objectID": "posts/startup-script/index.html#the-last-bit-of-convenience",
    "href": "posts/startup-script/index.html#the-last-bit-of-convenience",
    "title": "Python Startup Script",
    "section": "The last bit of convenience",
    "text": "The last bit of convenience\nWe got it, friends, the script works and the job is done. But… do you recall the last time you googled a shortcut for a semi-frequently used functionality? Do you still remember the shortcut, or use it each time you need the functionality? Convenience needs to be convenient (obviously). So let’s make the usage of our script as seamless as possible.\nIn Windows OS the best way for me would be to have a shortcut that would run it with one click. We could pin it to the taskbar and start menu. To create one:\n\nGo to any folder, right-click and select New &gt; Shortcut.\nIn the guide that pops up paste in %systemroot%\\System32\\cmd.exe /c \"python.exe $1\" where $1 should be replaced by the path to your script. You can also replace python.exe with interpreter path of any venv you wish.\nOptionally, you can later change the icon for the shortcut (needs to be .ico file). I like to roll with Win-98 style ‘My Computer’ icon.\nCopy the shortcut to start menu directory. For Win11 it’s C:\\ProgramData\\Microsoft\\Windows\\Start Menu\\Programs. You should be able to see it in start menu already. Now you can pin it to the taskbar.\n\n\nThat’s a good enough reminder and convenience tool for me. You might need some different setup, especially for another OS. For Linux, you might try to leverage activate or bashrc scripts in your working setup."
  },
  {
    "objectID": "posts/startup-script/index.html#further-extensions",
    "href": "posts/startup-script/index.html#further-extensions",
    "title": "Python Startup Script",
    "section": "Further extensions",
    "text": "Further extensions\nThere are a couple more things I wish this guy could do for me still:\n\nChanging Teams status to green - there’s a MS Graph API that could do this but we would need to manage Azure authorizations through the script. I am not up to that, especially on a work account. Might experiment on personal one in the future.\nDisplay placement of opened processes — I have 2 displays (which you could count as 4, due to the size format) and I am very particular about where each app should go. I would love the apps to open where they should. This is however OS specific, for Win it require to mingle with win32api. Yet another project for the future.\n\n\n\n\n\n\n\nNote\n\n\n\nDownload the startup script and example Windows shortcut."
  },
  {
    "objectID": "posts/exit-stack/index.html",
    "href": "posts/exit-stack/index.html",
    "title": "Exit stack to the rescue",
    "section": "",
    "text": "As a quantitative finance professional you’ll often find yourself with risk management systems (RMS). RMS’s are extensive frameworks that let you properly define a book (portfolio) of your financial transactions and run varia of pricing and risk analysis on it. For big financial players, like investment banks, the RMS will be internal proprietary codbase that is run in-house. For smaller enterprises or second-line reporting it’s not feasable to tackle creating such vast infrastructure. Hence, where there’s a need, someone will try to make money on it. This leads us to third-party (or vendor) RMS, of which there are plenty (ex. Murex, Acadia).\nWorking with vendor RMS, especially one that covers computations for you, entails juggling multiple resources to obtain your risk metrics. Defining OTC products, benchmarks, portfolios, and running risk analysis can involve numerous API calls, each requiring proper setup and cleanup. This can lead to messy code and potential errors or performance bottlenecks if resources aren’t handled correctly.\nThankfully, Python provides a powerful concept called context managers (CM) that streamline resource managment. True to the language’s ‘batteries included’ philosophy, there’s also a contextlib library that contains variety of tools for easing up your work with CMs. Today we’ll look at a (mock-up) usage of ExitStack class in real-life scenario of running risk analysis on RMS. If you need a refresher on CMs, check out this tutorial by RealPython."
  },
  {
    "objectID": "posts/exit-stack/index.html#working-with-risk-managment-systems",
    "href": "posts/exit-stack/index.html#working-with-risk-managment-systems",
    "title": "Exit stack to the rescue",
    "section": "",
    "text": "As a quantitative finance professional you’ll often find yourself with risk management systems (RMS). RMS’s are extensive frameworks that let you properly define a book (portfolio) of your financial transactions and run varia of pricing and risk analysis on it. For big financial players, like investment banks, the RMS will be internal proprietary codbase that is run in-house. For smaller enterprises or second-line reporting it’s not feasable to tackle creating such vast infrastructure. Hence, where there’s a need, someone will try to make money on it. This leads us to third-party (or vendor) RMS, of which there are plenty (ex. Murex, Acadia).\nWorking with vendor RMS, especially one that covers computations for you, entails juggling multiple resources to obtain your risk metrics. Defining OTC products, benchmarks, portfolios, and running risk analysis can involve numerous API calls, each requiring proper setup and cleanup. This can lead to messy code and potential errors or performance bottlenecks if resources aren’t handled correctly.\nThankfully, Python provides a powerful concept called context managers (CM) that streamline resource managment. True to the language’s ‘batteries included’ philosophy, there’s also a contextlib library that contains variety of tools for easing up your work with CMs. Today we’ll look at a (mock-up) usage of ExitStack class in real-life scenario of running risk analysis on RMS. If you need a refresher on CMs, check out this tutorial by RealPython."
  },
  {
    "objectID": "posts/exit-stack/index.html#setting-the-stage",
    "href": "posts/exit-stack/index.html#setting-the-stage",
    "title": "Exit stack to the rescue",
    "section": "Setting the stage",
    "text": "Setting the stage\nTo run an analysis, the RMS first needs to know what our positions are. In case of tradable assets it’s simple — we provide a market identifier and how much of the instrument we are holding. What do we do if we have some bespoke agreement with specific counterparty (an over-the-counter transaction)? We will need to define it from scratch in the RMS using data from the term sheet (assuming this kind of agreement is covered).\nNext, we need to specify the risk metrics we want to calculate — define the analysis scope. Let’s say we hold some equity options and we are intertested in their deltas and beta exposures. The betas are defined with respect to some benchmark — ex. portfolio holding 1 stock in US500 ETF. So we define the benchmark and link it to our analysis.\nFinally — once portfolio and analysis are defined in RMS — we call the API to start the calculation and respond with results. This is the control flow we execute to get to this point:\n\n\n\n\n\nflowchart LR\n  A[OTC Products] --&gt; B[Portfolio]\n  B --&gt; C{Analysis Run}\n  D[Benchmarks] --&gt; E[Analysis Definition]\n  E --&gt; C\n  C --&gt; F(Results)\n\n\n\n\n\n\nIf we know we’re never going to use all of the resources, we should clean up the server artifacts after receving the results. So for each resource we should have a CM.\n\nMock functions\nThe setup described above comes from a real-life situation I worked through. I can’t show you the actual API usage or data (or even the name of RMS itself), so we need to define some mocker functions. Mocks like this are actually not an uncommon thing — such approach is prevalent in testing API client code. In our case it would look like this:\n\nfrom enum import StrEnum\nfrom uuid import uuid4\n\n\nclass MockObject(StrEnum):\n    \"\"\"Types of mock objects.\"\"\"\n\n    ANALYSIS = \"analysis\"\n    BENCHMARK = \"benchmark\"\n    OTC_PRODUCTS = \"otc_products\"\n    PORTFOLIO = \"portfolio\"\n\n\ndef mock_object(object_type: MockObject) -&gt; str:\n    \"\"\"Mock a UUID for a given object type.\n\n    Args:\n        object_type: Type of object.\n    \"\"\"\n    return f\"{object_type}_{uuid4()}\"\n\n\ndef mock_preparation(object_type: MockObject, **kwargs) -&gt; None:\n    \"\"\"Mock preparation of an object.\n\n    Args:\n        object_type: Type of object.\n    \"\"\"\n    print(f\"Preparing {object_type}\" + (f\" using {kwargs}\" if kwargs else \".\"))\n\n\ndef mock_clean_up(object_uuid: str) -&gt; None:\n    \"\"\"Mock clean up of an object.\n\n    Args:\n        object_uuid: Uuid of the object.\n    \"\"\"\n    print(f\"Cleaning up after {object_uuid}.\")\n\nFor each of the four types of resources we mock the preparation, object (ex. API response, some id of definition on server) and the clean up process.\n\n\nContext managers\nEasiest way to define a CM is through contextlib.contextmanager decorator. To use it, you need a function that returns a generator. Code executed on enter should come before yield statement and the one for the exit afterwards. The generator yields the result of the CM (ex. handle to an opened file), the y in with x(*args) as y:.\n\nfrom contextlib import contextmanager\nfrom typing import Generator\n\n@contextmanager\ndef analysis(\n    *,\n    benchmark_uuid: str,\n) -&gt; Generator[str, None, None]:\n    \"\"\"Mock definition of an analysis.\n\n    Example: equity delta and correlation with benchmark.\n\n    Args:\n        benchmark_uuid: Uuid of the benchmark.\n    \"\"\"\n    mock_preparation(\n        MockObject.ANALYSIS,\n        benchmark_name=benchmark_uuid,\n    )\n    analysis_uuid = mock_object(MockObject.ANALYSIS)\n    yield analysis_uuid\n    mock_clean_up(analysis_uuid)\n\nModern approach to Python development leans heavily towards type annotations. Dynamical typing is powerful but can lead to unwieldy code. To properly annotate the analysis function we need to import Generator from typing module. Remember, the @contextmanager decorator takes the function and turns it into CM — a class with __enter__ and __exit__ methods. The Generator needs three inputs but in our case only the first one is important — YieldType, here str (see for more).\nWith this done implementing the 3 remaining CMs is easy, just remember our flow chart.\n\n@contextmanager\ndef benchmark() -&gt; Generator[str, None, None]:\n    \"\"\"Mock definition of a benchmark.\n\n    Args:\n        otc_products_uuid: Uuid of the otc products.\n    \"\"\"\n    mock_preparation(\n        MockObject.BENCHMARK,\n    )\n    benchmark_uuid = mock_object(MockObject.BENCHMARK)\n    yield benchmark_uuid\n    mock_clean_up(benchmark_uuid)\n\n\n@contextmanager\ndef otc_products() -&gt; Generator[str, None, None]:\n    \"\"\"Mock definition of an otc products.\n\n    Args:\n        otc_products_uuid: Uuid of the otc products.\n    \"\"\"\n    mock_preparation(MockObject.OTC_PRODUCTS)\n    otcs_uuid = mock_object(MockObject.OTC_PRODUCTS)\n    yield otcs_uuid\n    mock_clean_up(otcs_uuid)\n\n\n@contextmanager\ndef portfolio(\n    *,\n    portfolio_name: str,\n    otc_products_uuid: str,\n) -&gt; Generator[str, None, None]:\n    \"\"\"Mock definition of a portfolio.\n\n    Args:\n        otc_products_uuid: Uuid of the otc products.\n    \"\"\"\n    mock_preparation(\n        MockObject.PORTFOLIO,\n        portfolio_name=portfolio_name,\n        otc_products_uuid=otc_products_uuid,\n    )\n    portfolio_uuid = mock_object(MockObject.PORTFOLIO)\n    yield portfolio_uuid\n    mock_clean_up(portfolio_uuid)\n\n\n\nAnalysis results\nNo stress or complexity here, to run the analysis we need to specify which analysis to run on which portfolio.\n\nimport pandas as pd\n\ndef analysis_results(\n    *,\n    analysis_uuid: str,\n    portfolio_uuid: str,\n) -&gt; pd.DataFrame:\n    \"\"\"Mock running the analysis on a given portfolio.\n\n    Returns empty dataframe.\n\n    Args:\n        analysis_uuid: Uuid of the analysis.\n        portfolio_uuid: Uuid of the portfolio.\n    \"\"\"\n    print(f\"Running analysis {analysis_uuid} on portfolio {portfolio_uuid}.\")\n    return pd.DataFrame()"
  },
  {
    "objectID": "posts/exit-stack/index.html#section",
    "href": "posts/exit-stack/index.html#section",
    "title": "Exit stack to the rescue",
    "section": "",
    "text": "Finally, we can run some (mock) risk analysis!\n\nUsing contexts directly\nFirst, we use the managers directly through with clause, remembering the dependencies from our flow chart.\n\nPORTFOLIO = \"portfolio_1\"\n\ndef run_analysis() -&gt; pd.DataFrame:\n    \"\"\"Mock running the analysis using with clauses.\"\"\"\n    with otc_products() as otc_uuid:\n        with benchmark() as benchmark_uuid:\n            with portfolio(\n                portfolio_name=PORTFOLIO,\n                otc_products_uuid=otc_uuid,\n            ) as portfolio_uuid:\n                with analysis(\n                    benchmark_uuid=benchmark_uuid,\n                ) as analysis_uuid:\n                    results = analysis_results(\n                        analysis_uuid=analysis_uuid,\n                        portfolio_uuid=portfolio_uuid,\n                    )\n    return results\n\nThis is terrible! I am already getting lost, needed few tries to get it right. We ended up with 6 levels of indentation, the code is confusing, the flow is obtuse. Let’s run it either way, to see if at least works.\n\ndef print_title(title: str) -&gt; None:\n    \"\"\"Print a title padded, surrounded by dashes and empty lines.\"\"\"\n    print(\"\\n\" + title.center(60, \"-\") + \"\\n\")\n\nprint_title(\"Running analysis.\")\nrun_analysis()\n\n\n---------------------Running analysis.----------------------\n\nPreparing otc_products.\nPreparing benchmark.\nPreparing portfolio using {'portfolio_name': 'portfolio_1', 'otc_products_uuid': 'otc_products_0c4e8b1b-6051-459e-82bf-18d513f3c7c9'}\nPreparing analysis using {'benchmark_name': 'benchmark_80032ced-a130-46a9-8b8f-d51649b036b6'}\nRunning analysis analysis_6bff93be-802f-44ea-a575-5a4b3b9b278e on portfolio portfolio_6c80505c-7380-445c-bd3c-87cb8cb857af.\nCleaning up after analysis_6bff93be-802f-44ea-a575-5a4b3b9b278e.\nCleaning up after portfolio_6c80505c-7380-445c-bd3c-87cb8cb857af.\nCleaning up after benchmark_80032ced-a130-46a9-8b8f-d51649b036b6.\nCleaning up after otc_products_0c4e8b1b-6051-459e-82bf-18d513f3c7c9.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGreat, the behaviour is as expected, everything is cleaned after nicely. We achieved the goal but the code is unmaintainable. Looks like a subject of the joke “good code makes your job safe for a day, but terrible code in production makes it safe for a lifetime”. Being reckless and with no regard to job security as we are, we’ll fix it.\nI can clearly recall the most unamanagable and unreadable code I’ve seen in my career and the culprit was fired in the end. Different reasons, long time later, but still. So the joke is just a joke, don’t rely on a bad code as your job insurance."
  },
  {
    "objectID": "posts/exit-stack/index.html#the-exitstack",
    "href": "posts/exit-stack/index.html#the-exitstack",
    "title": "Exit stack to the rescue",
    "section": "The ExitStack",
    "text": "The ExitStack\nHere comes in the MVP — ExitStack from contextlib, made for streamlining complex context managment situationships. Conceptually it’s just a First-In-Last-Out (FILO) stack. You put CMs on top, one by one. When CM is pushed to stack, its __enter__ method is called and you can intercept the result. ExitStack is a CM itself, it’s __exit__ method is just calling the exits of CMs in reverse order.\n\n\n\n\n\nflowchart LR\n    A(Enter CM A) ---&gt; B(Enter CM B)\n    B ---&gt; C(Enter CM C)\n    C ---&gt; D[Do stuff]\n    D ---&gt; E(Exit CM C)\n    E ---&gt; F(Exit CM B)\n    F ---&gt; G(Exit CM A)\n    A -.- G\n    B -.- F\n    C -.- E\n\n\n\n\n\n\n\nSo the flow is exactly the same as in our first attempt. Let’s try it!\n\nfrom contextlib import ExitStack\n\ndef run_analysis_with_exit_stack() -&gt; None:\n    \"\"\"Mock running the analysis using exit stack.\"\"\"\n    with ExitStack() as stack:\n        otc_uuid = stack.enter_context(otc_products())\n        benchmark_uuid = stack.enter_context(benchmark())\n        portfolio_uuid = stack.enter_context(\n            portfolio(\n                portfolio_name=PORTFOLIO,\n                otc_products_uuid=otc_uuid,\n            )\n        )\n        analysis_uuid = stack.enter_context(\n            analysis(\n                benchmark_uuid=benchmark_uuid,\n            )\n        )\n        results = analysis_results(\n            analysis_uuid=analysis_uuid,\n            portfolio_uuid=portfolio_uuid,\n        )\n\nThat’s amazing (if the approach works)! In our code we end up with only single with clause and the outputs of CMs are defined just like the regular variables. We just need to wrap the CM calls in stack.enter_context method that pushes each CM to the stack.\n\nprint_title(\"Running analysis with exit stack.\")\nrun_analysis_with_exit_stack()\n\n\n-------------Running analysis with exit stack.--------------\n\nPreparing otc_products.\nPreparing benchmark.\nPreparing portfolio using {'portfolio_name': 'portfolio_1', 'otc_products_uuid': 'otc_products_481792c1-2e50-4d83-890b-a8cb6121bf13'}\nPreparing analysis using {'benchmark_name': 'benchmark_5206df24-8548-403c-90ab-505e3d9b6fd4'}\nRunning analysis analysis_24ad68ba-2568-4c2d-b104-f18df91e10ea on portfolio portfolio_2da70069-8d67-48ea-b2af-d10b10b773dc.\nCleaning up after analysis_24ad68ba-2568-4c2d-b104-f18df91e10ea.\nCleaning up after portfolio_2da70069-8d67-48ea-b2af-d10b10b773dc.\nCleaning up after benchmark_5206df24-8548-403c-90ab-505e3d9b6fd4.\nCleaning up after otc_products_481792c1-2e50-4d83-890b-a8cb6121bf13.\n\n\nIt works as well! We also get a package of benefits for free.\n\nDisabling the clean up\nWorking with API is tricky and debugging could be a painful experience. If we notice something iffy with the results we are reciving, it could be due to a bug at any of the stages. In such case disabling the artifact clean up and examining them is a good way to investigate. How do we do that? Comment out the exit code in our resource CMs? Nope, now we know better. With exit stack approach we just need to clean up the stack before exiting its context.\n\ndef run_analysis_with_exit_stack(\n    clean_up: bool = True,\n) -&gt; None:\n    \"\"\"Mock running the analysis using exit stack.\n\n    Args:\n        clean_up: Whether to clean up after the objects.\n    \"\"\"\n\n    with ExitStack() as stack:\n        otc_uuid = stack.enter_context(otc_products())\n        benchmark_uuid = stack.enter_context(benchmark())\n        portfolio_uuid = stack.enter_context(\n            portfolio(\n                portfolio_name=PORTFOLIO,\n                otc_products_uuid=otc_uuid,\n            )\n        )\n        analysis_uuid = stack.enter_context(\n            analysis(\n                benchmark_uuid=benchmark_uuid,\n            )\n        )\n        results = analysis_results(\n            analysis_uuid=analysis_uuid,\n            portfolio_uuid=portfolio_uuid,\n        )\n\n        if not clean_up:\n            _ = stack.pop_all()\n    return results\n\nThe _ = some_function() is a Pythonic way of disregarding outputs of some_function. Method pop_all actually moves the stack contents to a new stack, but we don’t care about that. We just want to get rid of them from our current one.\n\nprint_title(\"Running analysis with exit stack and no clean up.\")\nrun_analysis_with_exit_stack(clean_up=False)\n\n\n-----Running analysis with exit stack and no clean up.------\n\nPreparing otc_products.\nPreparing benchmark.\nPreparing portfolio using {'portfolio_name': 'portfolio_1', 'otc_products_uuid': 'otc_products_6314d748-8391-4c74-ab96-3387523b8e86'}\nPreparing analysis using {'benchmark_name': 'benchmark_ec868087-668a-4cae-a89b-2987c0f82590'}\nRunning analysis analysis_aa499825-ed45-4201-a261-d4bc9f23120e on portfolio portfolio_94294d81-6acc-4c15-b9cd-227741b6c89a.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultiple portfolios\nBenefit #2 — what do we do if we have multiple managers and many portfolios to re-run for? Or — outside of the example scope — we want to held multiple files open at the same time? Easy, we just push to the stack in a loop or a list comprehension.\n\nPORTFOLIOS = [\"portfolio_1\", \"portfolio_2\", \"portfolio_3\"]\n\ndef run_analysis_with_exit_stack(clean_up: bool = True):\n    \"\"\"Mock running the analysis for multiple portfolios using exit stack.\n\n    Args:\n        clean_up: Whether to clean up after the objects.\n    \"\"\"\n    with ExitStack() as stack:\n        otc_uuid = stack.enter_context(otc_products())\n        benchmark_uuid = stack.enter_context(benchmark())\n        portfolio_uuids = [\n            stack.enter_context(\n                portfolio(\n                    portfolio_name=portfolio_name,\n                    otc_products_uuid=otc_uuid,\n                )\n            )\n            for portfolio_name in PORTFOLIOS\n        ]\n        analysis_uuid = stack.enter_context(\n            analysis(\n                benchmark_uuid=benchmark_uuid,\n            )\n        )\n        result_parts = [\n            analysis_results(\n                analysis_uuid=analysis_uuid,\n                portfolio_uuid=portfolio_uuid,\n            )\n            for portfolio_uuid in portfolio_uuids\n        ]\n        results = pd.concat(result_parts)\n\n        if not clean_up:\n            _ = stack.pop_all()\n    return results\n\nprint_title(\"Running analysis with exit stack on multiple portfolios.\")\nrun_analysis_with_exit_stack(clean_up=True)\n\n\n--Running analysis with exit stack on multiple portfolios.--\n\nPreparing otc_products.\nPreparing benchmark.\nPreparing portfolio using {'portfolio_name': 'portfolio_1', 'otc_products_uuid': 'otc_products_15337209-1b80-4a0d-9923-8835208cd6a1'}\nPreparing portfolio using {'portfolio_name': 'portfolio_2', 'otc_products_uuid': 'otc_products_15337209-1b80-4a0d-9923-8835208cd6a1'}\nPreparing portfolio using {'portfolio_name': 'portfolio_3', 'otc_products_uuid': 'otc_products_15337209-1b80-4a0d-9923-8835208cd6a1'}\nPreparing analysis using {'benchmark_name': 'benchmark_5fe93e01-cd21-44c5-aff5-6c7db44d4276'}\nRunning analysis analysis_bfe28ed6-0fed-4801-8994-9a6f1515d9c6 on portfolio portfolio_bff801a9-18d1-4ac9-82d5-62e3e74fd469.\nRunning analysis analysis_bfe28ed6-0fed-4801-8994-9a6f1515d9c6 on portfolio portfolio_32f70938-5150-4866-b93f-3a2201e505e1.\nRunning analysis analysis_bfe28ed6-0fed-4801-8994-9a6f1515d9c6 on portfolio portfolio_40962dbb-9ffd-4684-8f09-5e3cb72b6187.\nCleaning up after analysis_bfe28ed6-0fed-4801-8994-9a6f1515d9c6.\nCleaning up after portfolio_40962dbb-9ffd-4684-8f09-5e3cb72b6187.\nCleaning up after portfolio_32f70938-5150-4866-b93f-3a2201e505e1.\nCleaning up after portfolio_bff801a9-18d1-4ac9-82d5-62e3e74fd469.\nCleaning up after benchmark_5fe93e01-cd21-44c5-aff5-6c7db44d4276.\nCleaning up after otc_products_15337209-1b80-4a0d-9923-8835208cd6a1."
  },
  {
    "objectID": "posts/exit-stack/index.html#conclusion",
    "href": "posts/exit-stack/index.html#conclusion",
    "title": "Exit stack to the rescue",
    "section": "Conclusion",
    "text": "Conclusion\nToday we’ve learnt a new Python tool and seen an example of how quantitative developer might set up risk reporting job on vendor RMS. Sound like a very niche and unlikely situation for you? Maybe. But the moral here is to go and explore the Python standard library. Without using any additional packages we improved readability and flexibility of our initial attempt. Python really has ‘batteries included’, see for yourself!\n\n\n\n\n\n\nNote\n\n\n\nDownload the whole code here."
  },
  {
    "objectID": "posts/linux-in-windows/index.html#windows-linux-and-you-ménage-à-trois",
    "href": "posts/linux-in-windows/index.html#windows-linux-and-you-ménage-à-trois",
    "title": "Linux in Windows via WSL",
    "section": "Windows, Linux, and You: Ménage à trois",
    "text": "Windows, Linux, and You: Ménage à trois\nTired of the same old Windows vs. Linux beef among PC superusers? Well, get ready to become a mediator in this feud. I’ll show you how to get the best of both worlds.\n\nPros and cons\nWindows OS has long been the dominant platform for mainstream consumers and businesses. It offers good hardware compatibility, de facto the PC gaming experience and is friendly to the casual user. However, Windows has significant limitations in terms of system control and software development tools.\nLinux, on the other hand, is an operating system created by developers and for developers (and system administrators). It provides infinite flexibility and a deep pool of open-source tools. But the learning curve is steep, the open-source projects often get abandoned and with great power (sudo) comes great responsibility (and sometimes system reinstall).\n\n\n\n\n\n\nNote\n\n\n\nActually, to publish this post in the most convenient and elegant way, I needed to switch to Linux. The Windows quarto CLI fails with some certification errors, coming from deno, that I can’t be bothered to clean up. Discovered this while writing these words as my previous approach was much messier. Irony is the ambrosia of life (for me at least).\n\n\n\n\nBridging the gap\nAt this point even Microsoft — a strong contrarian to Linux in the past — embraced the usefulness of Linux shell at your fingertips. Imagine having the familiar interface of Windows for your everyday tasks. Combine it with a the raw power and controllability of Linux for when you need to dive deep into development or system administration. And limited stakes once you (inevitably) remove half of your file system with a bash script. Enter Windows Subsystem for Linux (WSL): a ticket to 10x-ing your software development on Windows.\n\n\nWSL vs. Virtual Machines: What’s the Difference?\nYou might be wondering how WSL differs from a traditional virtual machine (VM). While both provide a way to run Linux on Windows, they operate in different ways. A VM emulates a virtual computer within your computer, complete with its own operating system and make-belive hardware resources. This makes VMs resource-intensive and can impact overall system performance. They are also hard to set up and maintain.\nWSL, on the other hand, is a more lightweight approach. It integrates Linux directly into the Windows kernel, allowing for faster boot times, better performance, and seamless file sharing between Windows and Linux environments."
  },
  {
    "objectID": "posts/linux-in-windows/index.html#installing-and-setting-up-wsl",
    "href": "posts/linux-in-windows/index.html#installing-and-setting-up-wsl",
    "title": "Linux in Windows via WSL",
    "section": "Installing and setting up WSL",
    "text": "Installing and setting up WSL\nFirst, we need to set up the WSL and install a chosen Linux distribution. Fortunately, this is “super easy, barely an inconvenience”. You can choose from a list of distros pre-packaged by Microsoft or download and build an image from scratch. You can even build Arch and be legally allowed to say BTW, I use Arch (on Windows). For this guide, we’ll take a more conventional approach and opt for the long-term support (LTS) version of Ubuntu. Now, open the Powershell with administrative privileges and run:\nwsl --update\nwsl --version\nThese commands ensure that WSL is up-to-date and displays the installed WSL version.\n\nOur next command will be:\nwsl -l -o\nThis yields a curated list of pre-packaged Linux distributions that Microsoft provides, making the process almost effortless.\n\nFor the installation itself:\nwsl --install -d 'Ubuntu-24.04'\n\nReboot the Windows system and you’ll see `Ubuntu’ among your applications."
  },
  {
    "objectID": "posts/linux-in-windows/index.html#base-setup-new-shell-and-package-manager",
    "href": "posts/linux-in-windows/index.html#base-setup-new-shell-and-package-manager",
    "title": "Linux in Windows via WSL",
    "section": "Base setup — new shell and package manager",
    "text": "Base setup — new shell and package manager\nThere is an additional benefit we can reap with just a bit of more work — a fully system-agnostic setup. By having Linux kernel run on Windows, we unified 2 out of 3 giants. The only pillar that is missing is the Mac OS. But wait, it’s based on Unix as well! It just uses different default shell and lacks a package manager, as Apple doesn’t trust its clients. zsh offers a more interactive and customizable shell experience compared to the default bash. Homebrew, often referred to as the “missing package manager for macOS”, provides a convenient way to install additional software on Linux systems. Let’s change the terminal shell to zsh in our Ubuntu installation and compile a secondary (to apt-get) package manager – brew.\n\nBrewing starts\nBefore we dive into installing Homebrew, we need to equip our system with the essential tools. The build-essential package provides a collection of compilers and libraries necessary for building software from the source code. Git is a version control system used for managing code projects.\nLet’s install these prerequisites using the following command:\nsudo apt-get upgrade -y\nsudo apt-get install build-essential git -y\nExecute the installation script for Homebrew:\nINSTALL_PATH=\"https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh\"\n/bin/bash -c \"$(curl -fsSL $INSTALL_PATH)\"\n\nTo make Homebrew accessible from your shell, you need to load its environment variables. The following command achieves this and subsequently runs a diagnostic check:\neval \"$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\"\nbrew doctor\n\nDoctor’s orders to append brew to some set of paths! Let’s do this then:\necho 'export XDG_DATA_DIRS=\"/home/linuxbrew/.linuxbrew/share:$XDG_DATA_DIRS\"' &gt;&gt; ~/.profile"
  },
  {
    "objectID": "posts/linux-in-windows/index.html#z-shell-sea-shell",
    "href": "posts/linux-in-windows/index.html#z-shell-sea-shell",
    "title": "Linux in Windows via WSL",
    "section": "Z-shell — sea-shell",
    "text": "Z-shell — sea-shell\nHomebrew makes package management a breeze. Let’s install zsh, a powerful and customizable shell.\nbrew install zsh\nzsh --version\nzsh\nThis will take you to interactive .zshrc file setup. The file contains commands that are run each time a shell is spawned. You can either generate empty file with 0 or go through the interactive setup with 1. \nAdd the brew initialization to .zshrc file as well. This way brew will always be by your side!\n(echo; echo 'eval \"$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\"') &gt;&gt; /home/bwrob/.zshrc\nCheck the default shell and set it to zsh:\necho $SHELL\nsudo chsh -s $(which zsh)\necho $SHELL"
  },
  {
    "objectID": "posts/linux-in-windows/index.html#github-command-line-tool",
    "href": "posts/linux-in-windows/index.html#github-command-line-tool",
    "title": "Linux in Windows via WSL",
    "section": "GitHub command line tool",
    "text": "GitHub command line tool\nRemember git != GitHub, we need a way to authenticate and use Github from CLI. There’s a tool for that!\nbrew install gh\ngh auth login\n\nNow create a convenient directory and copy your favourite GitHub repo:\nmkdir repos && cd repos\ngh repo clone python_playground"
  },
  {
    "objectID": "posts/linux-in-windows/index.html#connect-to-visual-studio-code",
    "href": "posts/linux-in-windows/index.html#connect-to-visual-studio-code",
    "title": "Linux in Windows via WSL",
    "section": "Connect to Visual Studio Code",
    "text": "Connect to Visual Studio Code\nWhile WSL grants us the power of Linux, let’s not forget the user-friendly interface of Windows. When it comes to software development, we can achieve the perfect blend by integrating Visual Studio Code (VSC) with our WSL environment.\nHere’s how:\n\nInstall the WSL Extension in VSC: Fire up VSC on your Windows machine and head over to the Extensions tab. Search for “WSL” and install the official extension by Microsoft: WSL extension.\nLaunching VSC from WSL: Within your WSL terminal, you can directly invoke VSC as if it were running natively on Windows. Use the following command:\n\ncode ~\\repos\\python_playground\n\nIn the next post I will show you how to set up Python environment in Linux. Stay tuned :) .\n\n\n\n\n\n\nNote\n\n\n\nDownload the Ubuntu shell script here."
  }
]